{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f00b7-255e-4ddc-83ac-fc439f2f4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf64d05-6e84-4bb2-9396-c8b0a57adda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT = \"Users\\kdfer\\Desktop\\ACM Research\\GalaxyImages\"\n",
    "\n",
    "ROOT = r\"C:\\Users\\kdfer\\Desktop\\ACM Research\\GalaxyImages\"\n",
    "SEED = 3126\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = .001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TARGET_MAP = {\n",
    "    \"Ellipticals\":0,\n",
    "    \"Irregulars\":1,\n",
    "    \"Lenticulars\":2,\n",
    "    \"Spirals\":3\n",
    "}\n",
    "path = r\"C:\\Users\\kdfer\\Desktop\\ACM Research\\GalaxyImages\\Ellipticals\"\n",
    "print(os.path.exists(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f24cb-9d3d-43d3-ba13-1e14192b15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [0.1462, 0.1318, 0.1607]\n",
    "stds  = [0.1302, 0.1230, 0.1437]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(.5),\n",
    "    transforms.RandomVerticalFlip(.5), \n",
    "    transforms.RandomAffine(degrees=0,\n",
    "                            translate=(32/224, 32/224),\n",
    "                            scale=(.85, 1.15),\n",
    "                            interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                           ),\n",
    "    transforms.ColorJitter(brightness=(.85, 1.35)),\n",
    "    transforms.RandomRotation(350,\n",
    "                              interpolation=transforms.InterpolationMode.BICUBIC\n",
    "                             ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d6f1e-1393-4387-9c93-8e042e1ad6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets all the data\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class GalaxiesData(Dataset):\n",
    "    def __init__(self, ROOT, transforms=None):\n",
    "        self.ROOT = os.path.normpath(ROOT)  # Normalize path\n",
    "        self.transforms = transforms\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for category in TARGET_MAP.keys():\n",
    "            category_path = os.path.join(self.ROOT, category)\n",
    "            category_path = os.path.normpath(category_path)  # Normalize again\n",
    "\n",
    "            if not os.path.exists(category_path):  # Check if path exists\n",
    "                print(f\"Warning: {category_path} does not exist.\")\n",
    "                continue\n",
    "\n",
    "            img_names = os.listdir(category_path)\n",
    "            for img_name in img_names:\n",
    "                img_path = os.path.join(category_path, img_name)\n",
    "                img_path = os.path.normpath(img_path)\n",
    "\n",
    "                self.imgs.append(img_path)\n",
    "                self.labels.append(TARGET_MAP[category])\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        print(f\"Loading image: {img_path}\")  # Debugging line\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416b7ff-ac83-4372-a2d4-1ab5b2e31c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the datasets and training/test stuff\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    seed = torch.initial_seed() % 2**32  # Generate unique seed\n",
    "    np.random.seed(seed)  # Set NumPy random seed\n",
    "    #print(f\"Worker {worker_id} initialized with seed {seed}\")\n",
    "\n",
    "train_dataset = GalaxiesData(ROOT, transforms=train_transforms)\n",
    "val_dataset   = GalaxiesData(ROOT, transforms=test_transforms)\n",
    "test_dataset  = GalaxiesData(ROOT, transforms=test_transforms)\n",
    "\n",
    "# Define data sizes\n",
    "train_size = int(0.7 * len(train_dataset))    # 70% for training\n",
    "val_size   = int(0.15 * len(train_dataset))   # 15% for validation\n",
    "test_size  = len(train_dataset) - train_size - val_size  # 15% for testing\n",
    "\n",
    "# Extract indices for training and validation+testing\n",
    "train_indices, val_test_indices = train_test_split(\n",
    "    range(len(train_dataset)), \n",
    "    test_size=(val_size + test_size), \n",
    "    shuffle=True, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Extract validation and test indices\n",
    "val_indices, test_indices = train_test_split(\n",
    "    val_test_indices, \n",
    "    test_size=test_size / (val_size + test_size), \n",
    "    shuffle=True, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Separate datasets using Subset\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset   = Subset(val_dataset, val_indices)\n",
    "test_subset  = Subset(test_dataset, test_indices)\n",
    "\n",
    "# Create DataLoader for each set\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, worker_init_fn=worker_init_fn)\n",
    "val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)\n",
    "test_loader  = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43830f3b-51ba-4064-9c72-89a7dba29390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, early_stopping_rounds=5, min_loss_change=0.001, verbose=False):\n",
    "        \"\"\"\n",
    "        An implementation of early stopping to restrain overfitting in the model. \n",
    "        \n",
    "            early_stopping_rounds (int): Number of epochs to wait after last improvement.\n",
    "            min_score_change (float): Minimum change in monitored metric to qualify as improvement.\n",
    "            verbose (bool): Whether to print early stopping messages.\n",
    "        \"\"\"\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        self.min_loss_change = min_loss_change\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        ## Improvement in loss has occurred; resets counter and saves current model\n",
    "        if val_loss < self.best_loss - self.min_loss_change:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  \n",
    "        ## No improvement in loss occurred; increases counter\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: [{self.counter}/{self.early_stopping_rounds}]\\n\")\n",
    "            if self.counter >= self.early_stopping_rounds:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a1f5b-f17b-4525-969b-22f8daf11bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total_samples = 0.0, 0.0, 0.0\n",
    "        \n",
    "    for imgs,labels in train_loader:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        ## Predicts logit\n",
    "        y_preds = model(imgs)\n",
    "        ## Computes loss\n",
    "        loss = loss_fn(y_preds, labels.long())\n",
    "        total_loss += loss.item()\n",
    "        ## Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_samples += labels.size(0)\n",
    "        preds = torch.argmax(y_preds, dim=1)\n",
    "        correct += torch.sum(preds==labels).item()\n",
    "        \n",
    "    accuracy = correct/total_samples\n",
    "    avg_loss = total_loss/len(train_loader)\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794fd8cd-89a1-4e93-aeb6-6b604070d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_fn, val_loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total_samples = 0.0, 0.0, 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for imgs,labels in val_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "    \n",
    "            y_preds = model(imgs)\n",
    "            loss = loss_fn(y_preds, labels.long())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "            preds = torch.argmax(y_preds, dim=1)\n",
    "            correct += torch.sum(preds==labels).item()\n",
    "\n",
    "        accuracy = correct/total_samples\n",
    "        avg_loss = total_loss/len(val_loader)\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eecc7f-62a1-4144-b1b7-62e061ecf8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_loop(epochs, model, optimizer, loss_fn, \n",
    "                        train_loader, val_loader, \n",
    "                        early_stopping_rounds=5, lr_schedule=None):\n",
    "    train_accuracies = []\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "\n",
    "    history = {}\n",
    "    early_stopping = EarlyStopping(early_stopping_rounds=early_stopping_rounds,\n",
    "                                   min_loss_change=0.001,\n",
    "                                   verbose=True)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        ### TRAINING PHASE\n",
    "        train_accuracy, train_loss = train(model, optimizer, loss_fn, train_loader)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"Training Epoch [{epoch}/{epochs}] - Accuracy: {train_accuracy:.4f} | Loss: {train_loss:.4f}\")\n",
    "\n",
    "        ### VALIDATION PHASE\n",
    "        val_accuracy, val_loss = validate(model, loss_fn, val_loader)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation - Accuracy: {val_accuracy:.4f} | Loss: {val_loss:.4f}\\n\")\n",
    "\n",
    "        ### EARLY STOPPING CHECK\n",
    "        early_stopping(model, val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "        ### LR SCHEDULER STEPPING\n",
    "        if lr_schedule is not None: lr_schedule.step()\n",
    "            \n",
    "    history[\"train_accuracies\"] = train_accuracies\n",
    "    history[\"val_accuracies\"]   = val_accuracies\n",
    "    history[\"train_losses\"] = train_losses\n",
    "    history[\"val_losses\"]   = val_losses\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a69e45-fdf6-41b5-ad15-5f9c7b7569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher model\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.2, num_classes=4):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = torch.hub.load(\"facebookresearch/hiera\", \n",
    "                                          model=\"hiera_base_224\", \n",
    "                                          pretrained=True, \n",
    "                                          checkpoint=\"mae_in1k_ft_in1k\")\n",
    "        self.head_features = 1000\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        ## Freeze transformer layers\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ## Function for creating linear blocks\n",
    "        def linearblock(in_features, out_features):\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(in_features, out_features, bias=False),\n",
    "                nn.BatchNorm1d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        ## Intermediate hidden layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            linearblock(self.head_features, 512),\n",
    "            #nn.Dropout(dropout),\n",
    "            linearblock(512, 256),\n",
    "            #nn.Dropout(dropout),\n",
    "            linearblock(256, 128),\n",
    "            #nn.Dropout(dropout),\n",
    "            linearblock(128, 8),\n",
    "            nn.Dropout(dropout/2),\n",
    "        )\n",
    "                \n",
    "        ## Final classification layer\n",
    "        self.classifier = nn.Linear(8, num_classes, bias=True)\n",
    "        ## Weight initialization\n",
    "        #self.init_layers_weights_()\n",
    "        \n",
    "    def init_layers_weights_(self):\n",
    "        for module in self.modules():\n",
    "            ## Kaiming Normal initialization for linear layers\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            ## Identity initialization for batchnorm layers\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a324f8-f89b-4880-899b-7dc21ed3b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872b96a-230c-4565-abf8-e2bf9510b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "\n",
    "model = TransformerClassifier().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train_validate_loop(\n",
    "    EPOCHS, \n",
    "    model,\n",
    "    optimizer, \n",
    "    loss_fn, \n",
    "    train_loader, \n",
    "    val_loader,\n",
    "    early_stopping_rounds=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945d0ea-d55a-477a-a79d-67726e73cb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
